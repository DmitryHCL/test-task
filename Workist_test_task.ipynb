{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a9ec8f-5754-4cd5-b262-c4a5df8a9a36",
   "metadata": {},
   "source": [
    "# Problem description: \n",
    "### Given document image, document PDF and OCR result create document classificator to predict 4 classes: resumee, email, invoice, letter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c209809-41a1-4cc1-8d6a-4a6da610ddb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Part 1 Approach\n",
    "# Part 2 Image classification model\n",
    "# Part 3 Text classification model\n",
    "# Part 4 Final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d716096-330b-4cf4-9687-804f1c790cb2",
   "metadata": {},
   "source": [
    "# Part 1 Approach\n",
    "### I will train image classification model and text classification model and combine results of both models to make final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e78c1033-76cb-4aa2-8eeb-ade4292e6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import efficientnet.tfkeras as efn \n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_auc_score\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.image import resize, decode_jpeg\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy.special import softmax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be317c21-21f9-457b-8a88-44534ad251b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('document_type_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab34e3dc-aa20-48de-9048-7b47d4486201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ocr</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'pageImages': [{'__typename': 'Image', 'width...</td>\n",
       "      <td>['Chaikin, ', 'Karen ', 'n ', \"O' \", 'o ', 'Fr...</td>\n",
       "      <td>email</td>\n",
       "      <td>2085136614c.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'pageImages': [{'__typename': 'Image', 'width...</td>\n",
       "      <td>['&gt; ', 'Jenny, ', 'After ', 'speaking ', 'with...</td>\n",
       "      <td>email</td>\n",
       "      <td>2085136814a.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'pageImages': [{'__typename': 'Image', 'width...</td>\n",
       "      <td>['Please ', 'call ', 'with ', 'any ', 'questio...</td>\n",
       "      <td>email</td>\n",
       "      <td>2085140145a.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'pageImages': [{'__typename': 'Image', 'width...</td>\n",
       "      <td>['2085158326 ', 'Williams, ', 'Carrie ', 'T. '...</td>\n",
       "      <td>email</td>\n",
       "      <td>2085158326.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'pageImages': [{'__typename': 'Image', 'width...</td>\n",
       "      <td>['GJ ', '□3 ', 'A ', 'nice ', 'ending ', 'to '...</td>\n",
       "      <td>email</td>\n",
       "      <td>2085161311b.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ocr  \\\n",
       "0  {'pageImages': [{'__typename': 'Image', 'width...   \n",
       "1  {'pageImages': [{'__typename': 'Image', 'width...   \n",
       "2  {'pageImages': [{'__typename': 'Image', 'width...   \n",
       "3  {'pageImages': [{'__typename': 'Image', 'width...   \n",
       "4  {'pageImages': [{'__typename': 'Image', 'width...   \n",
       "\n",
       "                                                text  label        file_name  \n",
       "0  ['Chaikin, ', 'Karen ', 'n ', \"O' \", 'o ', 'Fr...  email  2085136614c.pdf  \n",
       "1  ['> ', 'Jenny, ', 'After ', 'speaking ', 'with...  email  2085136814a.pdf  \n",
       "2  ['Please ', 'call ', 'with ', 'any ', 'questio...  email  2085140145a.pdf  \n",
       "3  ['2085158326 ', 'Williams, ', 'Carrie ', 'T. '...  email   2085158326.pdf  \n",
       "4  ['GJ ', '□3 ', 'A ', 'nice ', 'ending ', 'to '...  email  2085161311b.pdf  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5319b-a74a-458f-afc8-4458673f3030",
   "metadata": {},
   "source": [
    "### shuffle sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eca16ffd-eaa9-40fd-846e-8e5a03931560",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d848f-d071-4a8f-8d2b-1bc462ef3efd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 2: Image classification model\n",
    "### Many possible choises for the model\n",
    "### As a baseline I prefer efficientnet because there are several models to choose from with an increasing amount of parameters. \n",
    "### Depending on the speed vs quality needed for the task it I can choose whatever model I need. For the sake of speed in this example I will chose model with lowest parameters\n",
    "### I will use whole image as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "883d393f-287e-4d4f-8545-acd66e22a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create path to image that will be uaed as X\n",
    "df['image_path'] = df.label + '/' + df.file_name.str[:-4]+'.jpg'\n",
    "x_image = df['image_path']\n",
    "# labels\n",
    "y = pd.get_dummies(df['label']).astype('int32').values\n",
    "# train test split\n",
    "x_train, y_train = x_image[:80], y[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "580d0135-a211-4827-a8d6-92030dea0751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "img_size = 512 #arbitrary image size. In real life scenarios this will be one of the hyperparameters to tune.\n",
    "channels = 3\n",
    "batch_size = 16 \n",
    "epochs = 3 # for the sake of speed I will keep epochs count low. Normally I would use ~100 epochs and set callbacks to stop training whenever the model finds in global extrema\n",
    "labels = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9234dff7-347c-429e-b1a0-2160c17b1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that allows to iteratively read images from disk to avoid memory overflow\n",
    "def decode_image(filename, label=None, image_size=(img_size, img_size)): \n",
    "    bits = tf.io.read_file(filename)\n",
    "    image = decode_jpeg(bits, channels=channels)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = resize(image, image_size)\n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label\n",
    "\n",
    "# dedicated function for image augmentation. we can use any suitable augmentation here: image quality augmentations, rotations, etc.  \n",
    "def image_augment(filename, label): \n",
    "    image = decode_image(filename)\n",
    "    #image = tf.image.random_flip_left_right(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96afc678-b067-4ab8-a5b2-7b0b1428fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test pipeline\n",
    "train_ds = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .map(image_augment)\n",
    "    .cache()\n",
    "    .shuffle(16)\n",
    "    .repeat()\n",
    "    .batch(batch_size)\n",
    "    .prefetch(16)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f23bc29-e24e-4156-a645-ee051caf6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    base = efn.EfficientNetB0(weights='imagenet', input_shape=(img_size, img_size, channels), pooling = 'avg', include_top = False)\n",
    "    x = base.output\n",
    "    x = Dense(256, activation = 'elu')(x)\n",
    "    x = BatchNormalization(axis = -1)(x)\n",
    "    x = Dropout(rate = 0.5)(x)\n",
    "    x = Dense(256, activation = 'elu')(x)\n",
    "    x = BatchNormalization(axis = -1)(x)\n",
    "    output = Dense(labels, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=base.input, outputs = output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d8cc851-99e1-41ca-87ec-873bbcf7525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.0005)\n",
    "image_model = build_model()\n",
    "image_model.compile(optimizer=opt, loss=CategoricalCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35a8270-3a41-48bc-9594-7c3dcc17155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5/5 [==============================] - 31s 5s/step - loss: 1.1470 - accuracy: 0.5875\n",
      "Epoch 2/3\n",
      "5/5 [==============================] - 26s 5s/step - loss: 0.2672 - accuracy: 0.9250\n",
      "Epoch 3/3\n",
      "5/5 [==============================] - 26s 5s/step - loss: 0.2108 - accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "history = image_model.fit(\n",
    "            train_ds, \n",
    "            steps_per_epoch=x_train.shape[0] // batch_size, \n",
    "            epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c639b-a306-429f-b137-8d85a135e3c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 3: Language model for text\n",
    "### Once again, many possible choises for the model. From Naive Bayes to modern models like BERT. I will use smaller version of BERT - distilbert in this task\n",
    "### It might not the best possible choise here because I dont have enough memory to tokenize whole document without truncation and it is not what I would usually pick as a baseline model.\n",
    "### But I think that you dont need the whole document to make a correct prediction about its type. \n",
    "###  CV will usualy have a specific title in the beginning. Letters will have an adress etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "942eb93f-3866-48d3-82ff-5d75464e7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleat text from all non alphabetic + numeric symbols\n",
    "clean_text = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cb4db61-6719-42d2-adb5-c09ecc2568e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['full_text'] = df['text'].apply(clean_text.tokenize)\n",
    "df.full_text = df.full_text.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c77a68-0eea-45ca-a950-61376b9aa2bc",
   "metadata": {},
   "source": [
    "### This is the part that explains why throwing the whole document text is not the best solution. \n",
    "### Possible workaround: use OCR extracted coordinates to create \"phrases\" and feed phrases to the model. \n",
    "### This will increase our dataset, improve robustness of the model and will allow usage of more advanced and heavy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c11ea5fb-1e61-4d3e-a49f-cb088ccd2ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     100.000000\n",
       "mean     1079.770000\n",
       "std       671.980742\n",
       "min       177.000000\n",
       "25%       565.750000\n",
       "50%       911.000000\n",
       "75%      1443.250000\n",
       "max      3260.000000\n",
       "Name: full_text, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# too many words in the document\n",
    "df.full_text.map(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b0c2d8d-d37f-453c-9d19-1d950111beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text = df['full_text']\n",
    "y = pd.get_dummies(df['label']).astype('int32').values\n",
    "x_train, y_train = x_text[:80], y[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95ff5bf6-5240-48be-914e-f51e9e88ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 2\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "loss = CategoricalCrossentropy(from_logits=True)\n",
    "metric = CategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62d79265-011d-461f-b321-34a0f1e98994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'activation_13', 'vocab_layer_norm', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_20', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "max_length = 256 # maximum length of the string used\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(pretrained_model_name_or_path=model_name, do_lower_case=True)\n",
    "text_model = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=labels)\n",
    "text_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7c6f9d3-c71d-4b49-83eb-33c3d98d24d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer(\n",
    "    text=x_train.to_list(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_length,\n",
    "    truncation=True,\n",
    "    padding='max_length', \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b8164d3-998d-4f86-b8da-b97acaf2e97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5/5 [==============================] - 22s 3s/step - loss: 1.4073 - accuracy: 0.2625\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 17s 3s/step - loss: 0.9618 - accuracy: 0.5875\n"
     ]
    }
   ],
   "source": [
    "history = text_model.fit(\n",
    "    x=[x_train['input_ids'], x_train['attention_mask']],\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910966a5-30f4-472f-85bb-147cb46aa5b0",
   "metadata": {},
   "source": [
    "# Part 4: Final prediction\n",
    "### There are 2 ways to make the best out of 2 models\n",
    "### 1. Extract embedding feature layer from the CNN and BERT and train those outputs together using models like multinomial logistic regression or xgboost\n",
    "### 2. Combine probability predictions of 2 models with different coefficients and pick the class with the highest combined probability\n",
    "### I will use second approach in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "235629fc-13bf-4fca-aadc-1d17d83cca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image_test, x_text_test, y_test = x_image[80:], x_text[80:], y[80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51a821e8-558e-434c-ac8e-e104713ed683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ds = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_image_test, y_test))\n",
    "    .map(decode_image)\n",
    "    .cache()\n",
    "    .batch(batch_size)\n",
    "    .prefetch(16)\n",
    "    )\n",
    "x_test = tokenizer(\n",
    "    text=x_text_test.to_list(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_length,\n",
    "    truncation=True,\n",
    "    padding='max_length', \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c5027b0-a1dd-489a-a0c5-e63192d28c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pred = image_model.predict(test_ds)\n",
    "text_pred = text_model.predict(x=[x_test['input_ids'], x_test['attention_mask']])\n",
    "text_pred = softmax(text_pred[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9e22e89-b587-4248-a33a-fb26a3171463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(image_pred, text_pred, w1 = 0.5, w2 = 0.5):\n",
    "    num_predictions = len(image_pred)\n",
    "    a = np.zeros(shape=(num_predictions, labels))\n",
    "    for i in range(num_predictions):\n",
    "        for n in range(4):\n",
    "            a[i][n] = w1 * image_pred[i][n] + w2 * text_pred[i][n]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92d61d31-8266-47bb-bef3-13b9f659231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = ensemble(image_pred, text_pred)\n",
    "y_pred = (y_prob == y_prob.max(axis=1)[:,None]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd93a7d4-5cc5-438a-b1dd-e3cbabbe6892",
   "metadata": {},
   "source": [
    "### Not much to say about metrics since our dataset is too small to make significant conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9278af28-19d4-4a20-9923-91b3abefd94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         7\n",
      "           1       1.00      0.40      0.57         5\n",
      "           2       0.19      1.00      0.32         3\n",
      "           3       1.00      0.40      0.57         5\n",
      "\n",
      "   micro avg       0.35      0.35      0.35        20\n",
      "   macro avg       0.55      0.45      0.36        20\n",
      "weighted avg       0.53      0.35      0.33        20\n",
      " samples avg       0.35      0.35      0.35        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6591724f-6d18-4865-9cbb-94e2b428072f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
